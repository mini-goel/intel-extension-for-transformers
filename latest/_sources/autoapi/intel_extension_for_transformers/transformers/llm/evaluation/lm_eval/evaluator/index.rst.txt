:orphan:

:py:mod:`intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.evaluator`
========================================================================================

.. py:module:: intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.evaluator


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.evaluator.simple_evaluate
   intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.evaluator.evaluate



.. py:function:: simple_evaluate(model, model_args: Optional[Union[str, dict, object]] = None, tasks: Optional[List[Union[str, dict, object]]] = None, num_fewshot: Optional[int] = None, batch_size: Optional[int] = None, max_batch_size: Optional[int] = None, device: Optional[str] = None, use_cache: Optional[str] = None, cache_requests: bool = False, rewrite_requests_cache: bool = False, delete_requests_cache: bool = False, limit: Optional[Union[int, float]] = None, bootstrap_iters: int = 100000, check_integrity: bool = False, write_out: bool = False, log_samples: bool = True, gen_kwargs: Optional[str] = None, task_manager: Optional[lm_eval.tasks.TaskManager] = None, verbosity: str = 'INFO', predict_only: bool = False, random_seed: int = 0, numpy_random_seed: int = 1234, torch_random_seed: int = 1234, user_model: Optional[object] = None, tokenizer: Optional[object] = None)

   Instantiate and evaluate a model on a list of tasks.

   :param model: Union[str, LM]
       Name of model or LM object, see lm_eval.models.get_model
   :param model_args: Optional[str, dict]
       String or dict arguments for each model class, see LM.create_from_arg_string and LM.create_from_arg_object.
       Ignored if `model` argument is a LM object.
   :param tasks: list[Union[str, dict, Task]]
       List of task names or Task objects. Task objects will be taken to have name task.EVAL_HARNESS_NAME
       if defined and type(task).__name__ otherwise.
   :param num_fewshot: int
       Number of examples in few-shot context
   :param batch_size: int or str, optional
       Batch size for model
   :param max_batch_size: int, optional
       Maximal batch size to try with automatic batch size detection
   :param device: str, optional
       PyTorch device (e.g. "cpu" or "cuda:0") for running models
   :param use_cache: str, optional
       A path to a sqlite db file for caching model responses. `None` if not caching.
   :param cache_requests: bool, optional
       Speed up evaluation by caching the building of dataset requests. `None` if not caching.
   :param rewrite_requests_cache: bool, optional
       Rewrites all of the request cache if set to `True`. `None` if not desired.
   :param delete_requests_cache: bool, optional
       Deletes all of the request cache if set to `True`. `None` if not desired.
   :param limit: int or float, optional
       Limit the number of examples per task (only use this for testing), If <1,
       limit is a percentage of the total number of examples.
   :param bootstrap_iters:
       Number of iterations for bootstrap statistics
   :param check_integrity: bool
       Whether to run the relevant part of the test suite for the tasks
   :param write_out: bool
       If True, write out an example document and model input for checking task integrity
   :param log_samples: bool
       If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis
   :param gen_kwargs: str
       String arguments for model generation
       Ignored for all tasks with loglikelihood output_type
   :param predict_only: bool
       If true only model outputs will be generated and returned. Metrics will not be evaluated
   :param random_seed: int
       Random seed for python's random module. If set to None, the seed will not be set.
   :param numpy_random_seed: int
       Random seed for numpy. If set to None, the seed will not be set.
   :param torch_random_seed: int
       Random seed for torch. If set to None, the seed will not be set.

   :return
       Dictionary of results


.. py:function:: evaluate(lm: lm_eval.api.model.LM, task_dict, limit: Optional[int] = None, cache_requests: bool = False, rewrite_requests_cache: bool = False, bootstrap_iters: Optional[int] = 100000, write_out: bool = False, log_samples: bool = True, verbosity: str = 'INFO')

   Instantiate and evaluate a model on a list of tasks.

   :param lm: obj
       Language Model
   :param task_dict: dict[str, Task]
       Dictionary of tasks. Tasks will be taken to have name type(task).config.task .
   :param limit: int, optional
       Limit the number of examples per task (only use this for testing)
   :param bootstrap_iters:
       Number of iterations for bootstrap statistics
   :param write_out: bool
       If True, write out an example document and model input for checking task integrity
   :param log_samples: bool
       If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis
   :return
       Dictionary of results


