:orphan:

:py:mod:`intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.models.huggingface`
=================================================================================================

.. py:module:: intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.models.huggingface


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.transformers.llm.evaluation.lm_eval.models.huggingface.HFLM




.. py:class:: HFLM(pretrained: Optional[Union[str, transformers.PreTrainedModel]] = 'gpt2', backend: Optional[Literal[default, causal, seq2seq]] = 'default', revision: Optional[str] = 'main', subfolder: Optional[str] = None, tokenizer: Optional[Union[str, transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast]] = None, truncation: Optional[bool] = False, logits_cache: bool = True, max_length: Optional[int] = None, device: Optional[str] = 'cuda', dtype: Optional[Union[str, torch.dtype]] = 'auto', batch_size: Optional[Union[int, str]] = 1, max_batch_size: Optional[int] = 64, trust_remote_code: Optional[bool] = False, use_fast_tokenizer: Optional[bool] = True, add_bos_token: Optional[bool] = False, parallelize: Optional[bool] = False, device_map_option: Optional[str] = 'auto', max_memory_per_gpu: Optional[Union[int, str]] = None, max_cpu_memory: Optional[Union[int, str]] = None, offload_folder: Optional[Union[str, os.PathLike]] = './offload', peft: Optional[str] = None, autogptq: Optional[Union[bool, str]] = False, model_format: Optional[str] = 'torch', **kwargs)




   An abstracted Huggingface model class. Enables usage with both models of
   `intel_extension_for_transformers.transformers.AutoModelForCausalLM` and
   `intel_extension_for_transformers.transformers.AutoModelForSeq2SeqLM` classes.

   Supports data-parallel multi-GPU with HF Accelerate.


